# =============================================================================
# VARIABLES DE ENTORNO - ASISTENTE DE CONOCIMIENTO
# =============================================================================
# Copia este archivo a .env y ajusta los valores según tu entorno
# NUNCA incluyas .env en tu repositorio Git (ya está en .gitignore)
# =============================================================================

# -----------------------------------------------------------------------------
# CONFIGURACIÓN DE BASE DE DATOS
# -----------------------------------------------------------------------------
# Para desarrollo local: SQLite (no requiere instalación)
# Para producción: PostgreSQL o MySQL
DATABASE_URL=sqlite:///./database/asistente_conocimiento.db

# Ejemplos para otros entornos:
# DATABASE_URL=postgresql://user:password@localhost:5432/asistente_db
# DATABASE_URL=mysql://user:password@localhost:3306/asistente_db

# -----------------------------------------------------------------------------
# CONFIGURACIÓN DE SEGURIDAD - CRÍTICO
# -----------------------------------------------------------------------------
# SECRET_KEY: Genera una clave única con:
# python -c "import secrets; print(secrets.token_hex(32))"
# MÍNIMO 32 caracteres (64 caracteres hexadecimales)
SECRET_KEY=your-secret-key-here-replace-with-secure-random-value-min-64-chars

# Configuración JWT (JSON Web Tokens)
JWT_EXPIRATION_HOURS=24
JWT_ALGORITHM=HS256

# -----------------------------------------------------------------------------
# CONFIGURACIÓN DE OLLA E IA GENERATIVA
# -----------------------------------------------------------------------------
# Servidor local de Ollama (descarga desde: https://ollama.ai)
OLLAMA_HOST=http://localhost:11434

# Modelo de lenguaje recomendado para desarrollo
# Instala con: ollama pull llama3.1:8b-instruct-q4_K_M
OLLAMA_MODEL=llama3.1:8b-instruct-q4_K_M

# Parámetros del modelo LLM
LLM_TEMPERATURE=0.3        # 0.0 = más determinista, 2.0 = más creativo
LLM_MAX_TOKENS=500         # Máximo de tokens en respuestas
LLM_CONTEXT_SIZE=8192      # Tamaño del contexto para el modelo

# -----------------------------------------------------------------------------
# CONFIGURACIÓN DEL ENTORNO DE EJECUCIÓN
# -----------------------------------------------------------------------------
# Entorno actual: development, testing, production
FASTAPI_ENV=development

# Modo debug (solo para desarrollo)
DEBUG=True

# Nivel de logging: debug, info, warning, error
LOG_LEVEL=info

# -----------------------------------------------------------------------------
# CONFIGURACIÓN DE CORS (Cross-Origin Resource Sharing)
# -----------------------------------------------------------------------------
# Orígenes permitidos para acceder a la API (separados por comas)
# En producción, especificar solo los dominios confiables
ALLOWED_ORIGINS=http://localhost:5173,http://127.0.0.1:5173

# -----------------------------------------------------------------------------
# EJEMPLOS PARA DIFERENTES ENTORNOS
# -----------------------------------------------------------------------------

# Desarrollo (este archivo):
# FASTAPI_ENV=development
# DEBUG=True
# DATABASE_URL=sqlite:///./database/asistente_conocimiento.db

# Testing:
# FASTAPI_ENV=testing
# DEBUG=False
# DATABASE_URL=sqlite:///./test_database.db

# Producción con Docker:
# FASTAPI_ENV=production
# DEBUG=False
# DATABASE_URL=postgresql://user:password@database:5432/asistente_db
# OLLAMA_HOST=http://ollama:11434

# -----------------------------------------------------------------------------
# INSTRUCCIONES DE CONFIGURACIÓN
# -----------------------------------------------------------------------------
# 1. Copia este archivo: cp .env.example .env
# 2. Genera SECRET_KEY seguro y reemplaza el valor por defecto
# 3. Ajusta las variables según tu entorno
# 4. Inicia Ollama: ollama serve
# 5. Descarga el modelo: ollama pull llama3.1:8b-instruct-q4_K_M
# 6. Ejecuta la aplicación: python -m app.main
# =============================================================================
