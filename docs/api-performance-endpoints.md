# Performance & Metrics API Endpoints - Story 3.6

Esta documentación describe los nuevos endpoints para monitoreo de performance e información de caché, implementados en Story 3.6.

---

## Endpoints de Métricas de Performance

### GET /api/ia/metrics
Obtener estadísticas de performance y caché del sistema (últimas 24 horas).

**Autenticación:** Requerida (Admin role)
**Rate Limit:** 15 queries/60 segundos

**Headers:**
```
Authorization: Bearer {admin_token}
```

**Response 200 OK:**
```json
{
  "total_queries": 1250,
  "avg_response_time_ms": 1645.3,
  "p50_ms": 1500,
  "p95_ms": 2350,
  "p99_ms": 4200,
  "cache_hit_rate": 0.35,
  "avg_documents_retrieved": 2.8,
  "total_tokens_generated": 45000,
  "period_hours": 24,
  "generated_at": "2025-11-14T10:30:00Z"
}
```

**Response 403 Forbidden:**
```json
{
  "detail": {
    "code": "UNAUTHORIZED",
    "message": "User role 'user' does not have permission to access this resource"
  }
}
```

**Description:**
- **total_queries:** Total number of queries executed in period
- **avg_response_time_ms:** Average response time across all queries
- **p50_ms:** Median response time (50th percentile)
- **p95_ms:** 95th percentile response time
- **p99_ms:** 99th percentile response time
- **cache_hit_rate:** Proportion of queries served from cache (0.0-1.0)
- **avg_documents_retrieved:** Average number of source documents per query
- **total_tokens_generated:** Total tokens generated by LLM in period
- **period_hours:** Aggregation window (always 24 hours)
- **generated_at:** ISO 8601 timestamp of metric calculation

**Cálculation Details:**
- Aggregates data from `performance_metrics` table
- Window: Last 24 hours from current time
- Percentiles calculated using numpy-style quantile calculation
- Cache hit rate: `sum(cache_hit) / count(*)`

**Example Usage:**
```bash
curl -H "Authorization: Bearer {admin_token}" \
     https://api.example.com/api/ia/metrics
```

---

### GET /api/ia/health
Verificar estado del sistema incluyendo estadísticas de caché.

**Autenticación:** Requerida
**Rate Limit:** 20 queries/60 segundos

**Headers:**
```
Authorization: Bearer {token}
```

**Response 200 OK:**
```json
{
  "status": "ok",
  "model": "llama3.1:8b-instruct-q4_K_M",
  "ollama_version": "0.1.20",
  "cache_stats": {
    "cache_size": 45,
    "hit_rate": 0.35,
    "memory_usage_mb": 12.5,
    "response_cache_size": 30,
    "retrieval_cache_size": 15
  }
}
```

**Description:**
- **status:** "ok" if system operational, "error" otherwise
- **model:** Current LLM model name and version
- **ollama_version:** Version of Ollama runtime
- **cache_stats:**
  - **cache_size:** Total entries in both caches
  - **hit_rate:** Cache hit rate during session (0.0-1.0)
  - **memory_usage_mb:** Estimated memory consumed by caches
  - **response_cache_size:** Entries in response cache (0-100)
  - **retrieval_cache_size:** Entries in retrieval cache (0-100)

**Example Usage:**
```bash
curl -H "Authorization: Bearer {token}" \
     https://api.example.com/api/ia/health
```

---

## Parámetros de Configuración de Performance

Los siguientes parámetros pueden configurarse via variables de entorno (`.env` file):

### Cache Configuration
```bash
# Response cache TTL (seconds)
RESPONSE_CACHE_TTL_SECONDS=300

# Retrieval cache TTL (seconds)
RETRIEVAL_CACHE_TTL_SECONDS=600

# Maximum entries per cache
MAX_CACHE_SIZE=100
```

### Context Pruning
```bash
# Maximum tokens in context window
MAX_CONTEXT_TOKENS=2000
```

### Timeout Configuration
```bash
# Retrieval search timeout (milliseconds)
RETRIEVAL_TIMEOUT_MS=500

# LLM inference timeout (seconds)
LLM_INFERENCE_TIMEOUT_S=10
```

### Example .env File
```bash
# Performance Optimization (Story 3.6)
RESPONSE_CACHE_TTL_SECONDS=300
RETRIEVAL_CACHE_TTL_SECONDS=600
MAX_CACHE_SIZE=100
MAX_CONTEXT_TOKENS=2000
RETRIEVAL_TIMEOUT_MS=500
LLM_INFERENCE_TIMEOUT_S=10
```

---

## Performance Metrics Recording

### Query Response Enhancement (Story 3.6)

Cada query ahora incluye métricas de performance adicionales en la respuesta:

**Fields Adicionales en QueryResponse:**
```json
{
  "answer": "...",
  "sources": [...],
  "response_time_ms": 1245.5,
  "retrieval_time_ms": 95.2,
  "llm_time_ms": 1150.3,
  "cache_hit": false,
  "query": "user query text"
}
```

**New Fields:**
- **retrieval_time_ms:** Time spent in document retrieval phase
- **llm_time_ms:** Time spent in LLM inference
- **cache_hit:** Boolean indicating if response was from cache

**Example Request:**
```bash
curl -X POST https://api.example.com/api/ia/query \
  -H "Authorization: Bearer {token}" \
  -H "Content-Type: application/json" \
  -d '{"query": "How do I request leave?"}'
```

**Example Response:**
```json
{
  "answer": "To request leave, you should...",
  "sources": [
    {
      "document_id": 123,
      "title": "Leave Policy",
      "snippet": "Employees can request leave through..."
    }
  ],
  "response_time_ms": 645.3,
  "retrieval_time_ms": 82.4,
  "llm_time_ms": 562.9,
  "cache_hit": false,
  "query": "How do I request leave?"
}
```

---

## Performance Monitoring Best Practices

### 1. Regular Metric Review
- Check P95/P99 weekly for degradation
- Monitor cache hit rate trend
- Track total queries over time

### 2. Alerting Thresholds
Recommended alerting levels:
```
WARNING:  P95 > 2000ms
CRITICAL: P95 > 3000ms
WARNING:  Cache hit rate < 20%
CRITICAL: Cache hit rate < 10%
```

### 3. Optimization Actions
If performance degrades:
1. Check cache hit rate (is cache working?)
2. Review P99 vs P95 ratio (outliers?)
3. Check avg documents retrieved (large contexts?)
4. Monitor LLM time vs retrieval time ratio

### 4. Load Testing Approach
Before production:
```bash
# Test cache effectiveness
for i in {1..100}; do
  curl -H "Authorization: Bearer {token}" \
    -d "{\"query\": \"How to request leave?\"}" \
    https://api.example.com/api/ia/query
done
# Measure: Response times should decrease after first few queries

# Test under load
ab -n 100 -c 10 https://api.example.com/api/ia/metrics
# Verify: P95 remains <2.5 seconds under concurrency
```

---

## Troubleshooting

### High Response Times (>2500ms P95)

**Diagnosis:**
```bash
curl -H "Authorization: Bearer {admin_token}" \
     https://api.example.com/api/ia/metrics

# Check: Is llm_time_ms >> retrieval_time_ms?
# If yes: LLM is bottleneck (expected on CPU)
# If no: Check database/retrieval performance
```

**Solutions:**
1. Enable GPU acceleration for LLM
2. Reduce MAX_CONTEXT_TOKENS from 2000 to 1000
3. Use smaller model (q2_K instead of q4_K_M)

### Low Cache Hit Rate (<30%)

**Diagnosis:**
```bash
curl -H "Authorization: Bearer {admin_token}" \
     https://api.example.com/api/ia/metrics

# Check cache_hit_rate value
```

**Solutions:**
1. Increase RESPONSE_CACHE_TTL_SECONDS from 300 to 600
2. Analyze query logs for repetition patterns
3. Implement query normalization/fuzzy matching

### OOM (Out of Memory)

**Diagnosis:**
```bash
curl -H "Authorization: Bearer {token}" \
     https://api.example.com/api/ia/health

# Check cache_stats -> memory_usage_mb
```

**Solutions:**
1. Reduce MAX_CACHE_SIZE from 100 to 50
2. Reduce RESPONSE_CACHE_TTL_SECONDS
3. Monitor Ollama memory usage separately

---

## Integration Examples

### Python Client
```python
import requests

def get_metrics(admin_token: str) -> dict:
    """Fetch performance metrics from admin endpoint"""
    response = requests.get(
        'https://api.example.com/api/ia/metrics',
        headers={'Authorization': f'Bearer {admin_token}'}
    )
    response.raise_for_status()
    return response.json()

def check_health(token: str) -> dict:
    """Check system health including cache stats"""
    response = requests.get(
        'https://api.example.com/api/ia/health',
        headers={'Authorization': f'Bearer {token}'}
    )
    response.raise_for_status()
    return response.json()

# Usage
metrics = get_metrics(admin_token)
print(f"P95: {metrics['p95_ms']}ms")
print(f"Cache Hit Rate: {metrics['cache_hit_rate']*100:.1f}%")

health = check_health(token)
print(f"Cache Size: {health['cache_stats']['cache_size']}")
```

### JavaScript/TypeScript Client
```typescript
interface PerformanceMetrics {
  total_queries: number;
  avg_response_time_ms: number;
  p50_ms: number;
  p95_ms: number;
  p99_ms: number;
  cache_hit_rate: number;
  avg_documents_retrieved: number;
}

async function getMetrics(adminToken: string): Promise<PerformanceMetrics> {
  const response = await fetch('/api/ia/metrics', {
    headers: {
      'Authorization': `Bearer ${adminToken}`
    }
  });
  return response.json();
}

// Usage
const metrics = await getMetrics(adminToken);
console.log(`P95: ${metrics.p95_ms}ms`);
console.log(`Cache Hit Rate: ${(metrics.cache_hit_rate * 100).toFixed(1)}%`);
```

---

## API Rate Limits

| Endpoint | Limit | Window |
|----------|-------|--------|
| GET /api/ia/query | 10 | 60s per user |
| GET /api/ia/metrics | 15 | 60s per admin |
| GET /api/ia/health | 20 | 60s per user |

**Rate Limit Headers:**
```
X-RateLimit-Limit: 15
X-RateLimit-Remaining: 14
X-RateLimit-Reset: 1605537600
```

---

## Changelog

### Story 3.6 Updates

**Added:**
- GET /api/ia/metrics endpoint (admin)
- Cache statistics in GET /api/ia/health
- Performance fields in query response (retrieval_time_ms, llm_time_ms, cache_hit)
- Performance metrics recording to database

**Enhanced:**
- Query response schema with performance metadata
- Health endpoint with cache_stats object

**Configuration:**
- New .env variables for cache and timeout configuration
- Configurable context pruning token limit

---

**Document Status:** Final for Story 3.6
**Last Updated:** 2025-11-14
